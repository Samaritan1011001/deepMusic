Log 1
----------------------------------------------------------------------------------------------------------------------------------------

# Conv block 1
    x = layers.Convolution2D(16,(2, 2), border_mode='same', name='conv1',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(melgram_input)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn1', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool1')(x)
    x = layers.AlphaDropout(0.1, name='dropout1', trainable=True)(x)

    # Conv block 2
    x = layers.Convolution2D(32,(2, 2), border_mode='same', name='conv2',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn2', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')(x)
    x = layers.AlphaDropout(0.1, name='dropout2', trainable=True)(x)

    # # Conv block 3
    x = layers.Convolution2D(64,(3, 3), border_mode='same', name='conv3',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn3', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3')(x)
    x = layers.AlphaDropout(0.1, name='dropout3', trainable=True)(x)

    # GRU block 1, 2, output
    x = layers.GRU(8, return_sequences=True, name='gru1')(x)
    x = layers.GRU(8, return_sequences=False, name='gru2')(x)
    x = layers.AlphaDropout(0.3, name='final_drop')(x)

    x = layers.Dense(128, activation='relu', name='hidden1')(x)
    x = layers.Dense(4, activation='softmax', name='output')(x)

network_config = {
    'input_shape' : (1,96,1366),
    'loss' : 'categorical_crossentropy',
    'optimizer' : optimizers.Adam(learning_rate=0.001),
    'metrics' : ['acc'],
    'epochs' : 10,
    'batch_size':128,
}
test and val accuracy -> 70%

----------------------------------------------------------------------------------------------------------------------------------------
Log 2
----------------------------------------------------------------------------------------------------------------------------------------
 
# Conv block 1
    x = layers.Convolution2D(16,(2, 2), border_mode='same', name='conv1',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(melgram_input)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn1', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool1')(x)
    x = layers.AlphaDropout(0.1, name='dropout1', trainable=True)(x)

    # Conv block 2
    x = layers.Convolution2D(32,(2, 2), border_mode='same', name='conv2',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn2', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')(x)
    x = layers.AlphaDropout(0.1, name='dropout2', trainable=True)(x)

    # # Conv block 3
    x = layers.Convolution2D(32,(3, 3), border_mode='same', name='conv3',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn3', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3')(x)
    x = layers.AlphaDropout(0.1, name='dropout3', trainable=True)(x)
    
    # model = models.Model(melgram_input, x)
    # model.summary()

    # Conv block 4
    x = layers.Convolution2D(128,(4, 4), border_mode='same', name='conv4',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn4', trainable=True)(x)
    # x = layers.ELU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool4')(x)
    x = layers.Dropout(0.1, name='dropout4', trainable=True)(x)

    # reshaping
    if K.image_data_format() == 'channels_first':
        x = layers.Permute((3, 1, 2))(x)
    # print(f'permute size -> {x.shape}')
    shape = x.get_shape().as_list()
    x = layers.Reshape((shape[1]*shape[-1],shape[2]))(x)

    # GRU block 1, 2, output
    x = layers.GRU(16, return_sequences=True, name='gru1')(x)
    x = layers.GRU(16, return_sequences=False, name='gru2')(x)
    x = layers.AlphaDropout(0.3, name='final_drop')(x)

        x = layers.Dense(128, activation='relu', name='hidden1')(x)
        x = layers.Dense(4, activation='softmax', name='output')(x)

network_config = {
    'input_shape' : (1,96,1366),
    # 'input_shape' : (1,96,469),
    'loss' : 'categorical_crossentropy',
    'optimizer' : optimizers.Adam(learning_rate=0.0001),
    'metrics' : ['acc'],
    'epochs' : 15,
    'batch_size':128,

}

Testing accuracy ->  [0.8797454061398979, 0.699999988079071]



----------------------------------------------------------------------------------------------------------------------------------------
Log 3
----------------------------------------------------------------------------------------------------------------------------------------
 



# Conv block 1
    x = layers.Convolution2D(16,(2, 2), border_mode='same', name='conv1',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(melgram_input)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn1', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool1')(x)
    x = layers.AlphaDropout(0.1, name='dropout1', trainable=True)(x)

    # Conv block 2
    x = layers.Convolution2D(32,(2, 2), border_mode='same', name='conv2',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn2', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')(x)
    x = layers.AlphaDropout(0.1, name='dropout2', trainable=True)(x)

    # # Conv block 3
    x = layers.Convolution2D(32,(3, 3), border_mode='same', name='conv3',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn3', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3')(x)
    x = layers.AlphaDropout(0.1, name='dropout3', trainable=True)(x)
    
    # model = models.Model(melgram_input, x)
    # model.summary()

    # Conv block 4
    # x = layers.Convolution2D(64,(4, 4), border_mode='same', name='conv4',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    # x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn4', trainable=True)(x)
    # # x = layers.ELU()(x)
    # x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool4')(x)
    # x = layers.Dropout(0.1, name='dropout4', trainable=True)(x)

    # reshaping
    if K.image_data_format() == 'channels_first':
        x = layers.Permute((3, 1, 2))(x)
    # print(f'permute size -> {x.shape}')
    shape = x.get_shape().as_list()
    x = layers.Reshape((shape[1]*shape[-1],shape[2]))(x)

    # GRU block 1, 2, output
    x = layers.GRU(16, return_sequences=True, name='gru1')(x)
    x = layers.GRU(16, return_sequences=False, name='gru2')(x)
    x = layers.AlphaDropout(0.3, name='final_drop')(x)

    ## LSTM Layer
    # layer = LSTM(96,return_sequences=False)(x)
    # x = Dropout(0.4)(layer)
    # print(f'lstm layer -> {layer.shape}')

    if weights is None:
        x = layers.Dense(128, activation='relu', name='hidden1')(x)
        x = layers.Dense(4, activation='softmax', name='output')(x)


network_config = {
    'input_shape' : (1,96,1366),
    # 'input_shape' : (1,96,469),
    'loss' : 'categorical_crossentropy',
    'optimizer' : optimizers.Adam(learning_rate=0.0001),
    'metrics' : ['acc'],
    'epochs' : 20,
    'batch_size':128,

}

Testing accuracy ->  [0.7980081125070121, 0.7129771113395691]
Performed better during validation and training

----------------------------------------------------------------------------------------------------------------------------------------
Log 4
----------------------------------------------------------------------------------------------------------------------------------------
 
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 96, 1366)       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 16, 96, 1366)      80        
_________________________________________________________________
bn1 (BatchNormalization)     (None, 16, 96, 1366)      64        
_________________________________________________________________
pool1 (MaxPooling2D)         (None, 16, 24, 341)       0         
_________________________________________________________________
dropout1 (AlphaDropout)      (None, 16, 24, 341)       0         
_________________________________________________________________
conv2 (Conv2D)               (None, 32, 24, 341)       2080      
_________________________________________________________________
bn2 (BatchNormalization)     (None, 32, 24, 341)       128       
_________________________________________________________________
pool2 (MaxPooling2D)         (None, 32, 6, 85)         0         
_________________________________________________________________
dropout2 (AlphaDropout)      (None, 32, 6, 85)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 32, 6, 85)         9248      
_________________________________________________________________
bn3 (BatchNormalization)     (None, 32, 6, 85)         128       
_________________________________________________________________
pool3 (MaxPooling2D)         (None, 32, 3, 42)         0         
_________________________________________________________________
dropout3 (AlphaDropout)      (None, 32, 3, 42)         0         
_________________________________________________________________
permute_1 (Permute)          (None, 42, 32, 3)         0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 126, 32)           0         
_________________________________________________________________
gru1 (GRU)                   (None, 126, 16)           2352      
_________________________________________________________________
gru2 (GRU)                   (None, 16)                1584      
_________________________________________________________________
final_drop (AlphaDropout)    (None, 16)                0         
_________________________________________________________________
hidden1 (Dense)              (None, 128)               2176      
_________________________________________________________________
output (Dense)               (None, 4)                 516       
=================================================================
Total params: 18,356
Trainable params: 18,196
Non-trainable params: 160
_________________________________________________________________

# Conv block 1
    x = layers.Convolution2D(16,(2, 2), border_mode='same', name='conv1',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(melgram_input)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn1', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool1')(x)
    x = layers.AlphaDropout(0.1, name='dropout1', trainable=True)(x)

    # Conv block 2
    x = layers.Convolution2D(32,(2, 2), border_mode='same', name='conv2',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn2', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool2')(x)
    x = layers.AlphaDropout(0.1, name='dropout2', trainable=True)(x)

    # # Conv block 3
    x = layers.Convolution2D(32,(3, 3), border_mode='same', name='conv3',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn3', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3')(x)
    x = layers.AlphaDropout(0.1, name='dropout3', trainable=True)(x)
    
    # model = models.Model(melgram_input, x)
    # model.summary()


    # reshaping
    if K.image_data_format() == 'channels_first':
        x = layers.Permute((3, 1, 2))(x)
    # print(f'permute size -> {x.shape}')
    shape = x.get_shape().as_list()
    x = layers.Reshape((shape[1]*shape[-1],shape[2]))(x)

    # GRU block 1, 2, output
    x = layers.GRU(16, return_sequences=True, name='gru1')(x)
    x = layers.GRU(16, return_sequences=False, name='gru2')(x)
    x = layers.AlphaDropout(0.3, name='final_drop')(x)

    ## LSTM Layer
    # layer = LSTM(96,return_sequences=False)(x)
    # x = Dropout(0.4)(layer)
    # print(f'lstm layer -> {layer.shape}')

    if weights is None:
        x = layers.Dense(128, activation='relu', name='hidden1')(x)
        x = layers.Dense(4, activation='softmax', name='output')(x)
        print(f'x.shape -> {x.shape}')

network_config = {
    'input_shape' : (1,96,1366),
    # 'input_shape' : (1,96,469),
    'loss' : 'categorical_crossentropy',
    'optimizer' : optimizers.Adam(learning_rate=0.0001),
    'metrics' : ['acc'],
    'epochs' : 20,
    'batch_size':128,

}

Testing accuracy ->  [0.8126908710894694, 0.7076336145401001]

----------------------------------------------------------------------------------------------------------------------------------------
Log 5
----------------------------------------------------------------------------------------------------------------------------------------
 Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 96, 1366)       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 16, 96, 1366)      80        
_________________________________________________________________
bn1 (BatchNormalization)     (None, 16, 96, 1366)      64        
_________________________________________________________________
pool1 (MaxPooling2D)         (None, 16, 24, 341)       0         
_________________________________________________________________
dropout1 (AlphaDropout)      (None, 16, 24, 341)       0         
_________________________________________________________________
conv2 (Conv2D)               (None, 32, 24, 341)       2080      
_________________________________________________________________
bn2 (BatchNormalization)     (None, 32, 24, 341)       128       
_________________________________________________________________
pool2 (MaxPooling2D)         (None, 32, 6, 85)         0         
_________________________________________________________________
dropout2 (AlphaDropout)      (None, 32, 6, 85)         0         
_________________________________________________________________
conv3 (Conv2D)               (None, 64, 6, 85)         18496     
_________________________________________________________________
bn3 (BatchNormalization)     (None, 64, 6, 85)         256       
_________________________________________________________________
pool3 (MaxPooling2D)         (None, 64, 2, 28)         0         
_________________________________________________________________
dropout3 (AlphaDropout)      (None, 64, 2, 28)         0         
_________________________________________________________________
permute_1 (Permute)          (None, 28, 64, 2)         0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 56, 64)            0         
_________________________________________________________________
gru1 (GRU)                   (None, 56, 16)            3888      
_________________________________________________________________
gru2 (GRU)                   (None, 16)                1584      
_________________________________________________________________
final_drop (AlphaDropout)    (None, 16)                0         
_________________________________________________________________
hidden1 (Dense)              (None, 128)               2176      
_________________________________________________________________
output (Dense)               (None, 4)                 516       
=================================================================
Total params: 29,268
Trainable params: 29,044
Non-trainable params: 224
_________________________________________________________________

# Conv block 1
    x = layers.Convolution2D(16,(2, 2), border_mode='same', name='conv1',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(melgram_input)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn1', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool1')(x)
    x = layers.AlphaDropout(0.1, name='dropout1', trainable=True)(x)

    # Conv block 2
    x = layers.Convolution2D(32,(2, 2), border_mode='same', name='conv2',activation= 'selu', trainable=True,kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn2', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool2')(x)
    x = layers.AlphaDropout(0.1, name='dropout2', trainable=True)(x)

    # # Conv block 3
    x = layers.Convolution2D(64,(3, 3), border_mode='same', name='conv3',activation= 'selu', trainable=True, kernel_initializer='lecun_normal')(x)
    x = layers.BatchNormalization(axis=channel_axis, mode=0, name='bn3', trainable=True)(x)
    # x = layers.ReLU()(x)
    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name='pool3')(x)
    x = layers.AlphaDropout(0.1, name='dropout3', trainable=True)(x)
    
    # model = models.Model(melgram_input, x)
    # model.summary()


    # reshaping
    if K.image_data_format() == 'channels_first':
        x = layers.Permute((3, 1, 2))(x)
    # print(f'permute size -> {x.shape}')
    shape = x.get_shape().as_list()
    x = layers.Reshape((shape[1]*shape[-1],shape[2]))(x)

    # GRU block 1, 2, output
    x = layers.GRU(16, return_sequences=True, name='gru1')(x)
    x = layers.GRU(16, return_sequences=False, name='gru2')(x)
    x = layers.AlphaDropout(0.3, name='final_drop')(x)

    ## LSTM Layer
    # layer = LSTM(96,return_sequences=False)(x)
    # x = Dropout(0.4)(layer)
    # print(f'lstm layer -> {layer.shape}')

    if weights is None:
        x = layers.Dense(128, activation='relu', name='hidden1')(x)
        x = layers.Dense(4, activation='softmax', name='output')(x)

network_config = {
    'input_shape' : (1,96,1366),
    # 'input_shape' : (1,96,469),
    'loss' : 'categorical_crossentropy',
    'optimizer' : optimizers.Adam(learning_rate=0.0001),
    'metrics' : ['acc'],
    'epochs' : 20,
    'batch_size':128,

}

Testing accuracy ->  [0.7891382686054433, 0.7206106781959534]


