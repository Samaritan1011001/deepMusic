{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import pydot\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import multiprocessing.sharedctypes as sharedctypes\n",
    "import os.path\n",
    "import ast\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from python_speech_features import mfcc, logfbank\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import models\n",
    "# from tensorflow.keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def build_sample_loader(audio_dir, Y, loader):\n",
    "\n",
    "    class SampleLoader:\n",
    "\n",
    "        def __init__(self, tids, batch_size=4):\n",
    "#             self.lock1 = multiprocessing.Lock()\n",
    "#             self.lock2 = multiprocessing.Lock()\n",
    "            self.batch_foremost = 0\n",
    "#             self.batch_rearmost = sharedctypes.RawValue(ctypes.c_int, -1)\n",
    "#             self.condition = multiprocessing.Condition(lock=self.lock2)\n",
    "\n",
    "#             data = sharedctypes.RawArray(ctypes.c_int, tids.data)\n",
    "#             self.tids = np.ctypeslib.as_array(data)\n",
    "            self.tids = np.asarray(tids.data)\n",
    "#             print(f'self.tids-> {type(self.tids)}')\n",
    "\n",
    "\n",
    "            self.batch_size = batch_size\n",
    "            self.loader = loader\n",
    "            self.X = []\n",
    "            self.Y = []\n",
    "\n",
    "        def __iter__(self):\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "\n",
    "#             with self.lock1:\n",
    "            if self.batch_foremost == 0:\n",
    "                np.random.shuffle(self.tids)\n",
    "\n",
    "            batch_current = self.batch_foremost\n",
    "            if self.batch_foremost + self.batch_size < self.tids.size:\n",
    "                batch_size = self.batch_size\n",
    "                self.batch_foremost += self.batch_size\n",
    "            else:\n",
    "                batch_size = self.tids.size - self.batch_foremost\n",
    "                self.batch_foremost = 0\n",
    "\n",
    "            tids = np.array(self.tids[batch_current:batch_current+batch_size])\n",
    "\n",
    "#             print(tids)\n",
    "            for i, tid in enumerate(tids):\n",
    "#                 self.X[i] = self.loader.load(get_audio_path(audio_dir, tid))\n",
    "                signal, rate = self.loader.load(utils.get_audio_path(audio_dir, tid))\n",
    "#                 print(f'signal -> {signal.shape}')\n",
    "#                 print(f'rate -> {rate}')\n",
    "#                 print(f'signal rate -> {signal[:rate].shape}')\n",
    "#                 print(f'self.x -> {self.X[i].shape}')\n",
    "#                 print(f'mfcc -> {mfcc(signal[:rate],rate, numcep=13, nfilt=26, nfft=1103).T.shape}')\n",
    "                ran_index = np.random.randint(0,signal.shape[0]-int(rate/10))\n",
    "                sample = signal[ran_index:ran_index+int(rate/10)]\n",
    "                normalized_X = preprocessing.normalize(mfcc(sample,22050,numcep=13, nfilt=26, nfft=1103).T)\n",
    "#                 print(f'norm x shape {normalized_X.shape}')\n",
    "                self.X.append(normalized_X)\n",
    "                self.Y.append(Y.loc[tid])\n",
    "\n",
    "            temp = np.array(self.X[:batch_size])\n",
    "#             print(f'temp x shape {temp.shape}')\n",
    "            rshaped_X = temp.reshape(temp.shape[0],temp.shape[1],temp.shape[2],1)\n",
    "            return rshaped_X , np.array(self.Y[:batch_size])\n",
    "\n",
    "    return SampleLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "\n",
    "tracks = utils.load('fma_metadata/tracks.csv')\n",
    "features = utils.load('fma_metadata/features.csv')\n",
    "echonest = utils.load('fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()\n",
    "\n",
    "tracks.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = tracks.index[tracks['set', 'subset'] == 'small']\n",
    "\n",
    "assert subset.isin(tracks.index).all()\n",
    "assert subset.isin(features.index).all()\n",
    "\n",
    "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "features_all = features.loc[subset]\n",
    "\n",
    "tracks.shape, features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n",
    "\n",
    "genres = list(LabelEncoder().fit(tracks['track', 'genre_top']).classes_)\n",
    "#genres = list(tracks['track', 'genre_top'].unique())\n",
    "print('Top genres ({}): {}'.format(len(genres), genres))\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n",
    "print('All genres ({}): {}'.format(len(genres), genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = utils.FfmpegLoader()\n",
    "SampleLoader = build_sample_loader(AUDIO_DIR, labels_onehot, loader)\n",
    "print('Dimensionality: {}'.format(loader.shape))\n",
    "sample_X_shape = SampleLoader(train, batch_size=10).__next__()[0].shape\n",
    "sample_X_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "# from tensorflow.keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "input_shape = (sample_X_shape[1],sample_X_shape[2],1)\n",
    "print(input_shape)\n",
    "# keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16,(3,3),activation = 'relu',\n",
    "                        strides=(1,1), padding='same',\n",
    "                        input_shape=input_shape))\n",
    "model.add(layers.Conv2D(32,(3,3),activation = 'relu',\n",
    "                        strides=(1,1), padding='same',\n",
    "                        ))\n",
    "model.add(layers.Conv2D(64,(3,3),activation = 'relu',\n",
    "                        strides=(1,1), padding='same',\n",
    "                        ))\n",
    "model.add(layers.Conv2D(128,(3,3),activation = 'relu',\n",
    "                        strides=(1,1), padding='same',\n",
    "                        ))\n",
    "model.add(layers.MaxPool2D(2,2))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=1), steps_per_epoch=len(train) // 1, epochs=1)\n",
    "loss = model.evaluate_generator(SampleLoader(val, batch_size=10), val.size)\n",
    "# loss = model.evaluate_generator(SampleLoader(test, batch_size=10), test.size)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
